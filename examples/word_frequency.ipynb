{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example you will learn how to:\n",
    "\n",
    "1. Read the article data\n",
    "2. Given some date range, find articles in that range\n",
    "3. Find the frequent words in the specified articles\n",
    "4. Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read article data from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to corpus directory; change this value as necessary\n",
    "directory_path = '../corpus'\n",
    "\n",
    "# read all files\n",
    "all_files = [f for f in listdir(directory_path) if isfile(join(directory_path, f))]\n",
    "\n",
    "# create a list to hold data\n",
    "articles = []\n",
    "\n",
    "# iterate over each csv file\n",
    "for f in all_files:\n",
    "\n",
    "    rows = 0\n",
    "\n",
    "    # open the file for reading\n",
    "    with open(join(directory_path, f)) as csvfile:\n",
    "\n",
    "        # read file contents\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in readCSV:\n",
    "\n",
    "            # skip header row\n",
    "            if rows > 0:\n",
    "                articles.append(row)\n",
    "\n",
    "            rows += 1\n",
    "\n",
    "print('Sanity check! Got', len(articles), 'articles.')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Add some helper functions to make life easier later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of each of these functions is described in the docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(string_date):\n",
    "    \"\"\"This function converts string timestamp to a datetime, and zeros out the time (hours, min, etc.)\"\"\"\n",
    "    date_format = '%Y-%m-%dT%H:%M:%S.%f%z' if 'T' in string_date else '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "    return (datetime.strptime(string_date, date_format)) \\\n",
    "        .replace(hour=0, minute=0, second=0, microsecond=0).date()\n",
    "\n",
    "\n",
    "def longest_base_word(word_list):\n",
    "    \"\"\"Given a list of words, this function finds the longest common sequence from\n",
    "    the beginning that is common between each words in the list\"\"\"\n",
    "\n",
    "    unique_words = list(set(word_list))\n",
    "\n",
    "    # if list contains exactly 1 words, return that\n",
    "    if len(unique_words) == 1:\n",
    "        return unique_words[0]\n",
    "\n",
    "    max_len, res = -1, ['']\n",
    "\n",
    "    # find longest word\n",
    "    for ele in word_list:\n",
    "        if len(ele) > max_len:\n",
    "            max_len = len(ele)\n",
    "\n",
    "    # loop substrings        \n",
    "    for idx in range(1, max_len):\n",
    "        tmp = list(set([t[0:idx] for t in word_list]))\n",
    "        if len(tmp) > 1:\n",
    "            break\n",
    "        res = tmp\n",
    "    return res[0]\n",
    "\n",
    "\n",
    "def most_frequent(word_list, n):\n",
    "    \"\"\"Given a list of non-unique strings, return top n most frequent strings\"\"\"\n",
    "    \n",
    "    words = dict.fromkeys(list(set(word_list)), 0)\n",
    "    \n",
    "    for k in words.keys():\n",
    "        words[k] = len([w for w in word_list if w == k])\n",
    "    \n",
    "    return [i[1] for i in sorted([(v, k) for k, v in words.items()], reverse=True)][0:n]\n",
    "\n",
    "\n",
    "def strip_nonalpha(str):\n",
    "    \"\"\"Remove non-alphabetic characters, except spaces\"\"\"\n",
    "    return ''.join([c for c in list(str.lower()) if c in [' ', 'ä', 'ö'] or (ord('a') <= ord(c) <= ord('z'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create date range for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to study articles from March 12, 2020 (because it is the last full date of data in the corpus at the time of writing this), and going backwards by 90 days. You may adjust these limits however you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from March 12, 2020\n",
    "base = datetime(2020, 3, 12)\n",
    "\n",
    "# make a list of dates 90 days prior; zeroing out the time part\n",
    "num_days = 90\n",
    "\n",
    "date_list = [(base - timedelta(days=x))\n",
    "             .replace(hour=0, minute=0, second=0, microsecond=0).date() \n",
    "             for x in range(0, num_days)]\n",
    "\n",
    "print('Sanity check! Analyzing dates:', date_list[0], 'through', date_list[-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find words in article titles occurring within the date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify minimum word length\n",
    "# we will ignore all words that are shorter than this length!\n",
    "# This will eliminate many stop words\n",
    "min_word_len = 4\n",
    "\n",
    "# ignore rare words that occurred fewer times than this:\n",
    "# (adjust this limit as necessary)\n",
    "min_occurrence = 100\n",
    "\n",
    "# Make a dictionary to hold temporary results\n",
    "words = {}\n",
    "\n",
    "# loop articles where publish date is within date range\n",
    "for article in [a for a in articles if parse_date(a[0]) in date_list]:\n",
    "\n",
    "    # sanitize article title, split into words\n",
    "    title_words = strip_nonalpha(article[1]).split(' ')\n",
    "\n",
    "    for w in title_words:\n",
    "        \n",
    "        # ignore short words\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "            \n",
    "        # get base word    \n",
    "        _key = w[0:min_word_len]\n",
    "        \n",
    "        # add word to our dictionary\n",
    "        if _key in words:\n",
    "            words[_key].append(w)\n",
    "        else:\n",
    "            words[_key] = [w]\n",
    "\n",
    "# Make a list of most popular words\n",
    "# key (k) - represents a base word, length of this key is equal to min_word_len\n",
    "# value (v) - a list of actual words that match that base word, and may contain duplicates\n",
    "# len(v) - the total number of occurrences that were found within the specified date range\n",
    "# reverse=True - gives use descending order, i.e. highest frequency items first\n",
    "top_words = sorted([(len(v), k, v) for k, v in words.items() if len(v) > min_occurrence], reverse=True)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(top_words)):\n",
    "    \n",
    "    # unpack values\n",
    "    (freq, k, v) = top_words[i]\n",
    "    \n",
    "    # longest commmon base word\n",
    "    base_word = longest_base_word(v)\n",
    "    # number of diffrent words that share the same base\n",
    "    unique_word_count = len(list(set(v)))\n",
    "    # top 5 most frequent words\n",
    "    full_words = most_frequent(v, 5)\n",
    "    \n",
    "    # format output\n",
    "    display = lambda x,y : str(x).ljust(y, ' ')\n",
    "\n",
    "    # display row of data\n",
    "    print(display(str(i + 1) + '.', 5),\n",
    "          display(base_word, 10),\n",
    "          display(freq, 5),\n",
    "          display(unique_word_count, 5),\n",
    "          ', '.join(full_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "print('IL Suosituimmat jutun aiheet', date_list[-1], '-', date_list[0], '\\n')\n",
    "\n",
    "for entry in top_words[0: 20]:\n",
    "    print(str(n).ljust(3,' '), ',\\n'.join(most_frequent(entry[-1], 1)))\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [&laquo; Previous Lab](getting_started.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
